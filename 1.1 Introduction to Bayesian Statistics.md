# 1.1 Introduction to Bayesian Statistics

## Bayesian vs. Classical Inference
- **Classical Approach**:
  - Parameters are considered fixed but unknown constants.
  - Inference is based on the likelihood function \( f(x|\theta) \), where \( \theta \) is the parameter.
  - Example: Maximum Likelihood Estimation (MLE).
- **Bayesian Approach**:
  - Parameters are treated as random variables with unknown distributions.
  - Inference is based on the posterior distribution \( \pi(\theta|x) \), which represents the updated beliefs about the parameters given the data.
  - The posterior distribution combines prior beliefs (prior distribution) and data information (likelihood).

## The Role of Prior Distributions
- **Prior Distribution**: Represents initial beliefs about the parameters before observing any data.
  - Can be based on expert knowledge, historical data, or subjective judgment.
- **Posterior Distribution**: Updated distribution of the parameters after incorporating the observed data.
  - Combines prior knowledge and data information through Bayes' Theorem.

## Advantages and Criticisms of Bayesian Methods
- **Advantages**:
  - Incorporates prior knowledge formally and robustly.
  - Provides a natural way to update beliefs as new data become available.
- **Criticisms**:
  - The choice of prior distribution is subjective and can influence the results.
  - Different priors can lead to different inferences, which some view as a drawback.

## Example: Bayesian Inference in Everyday Life
- Suppose you see a large wooden object with green branches outside your window.
  - You might consider two possibilities: it is a tree (A₁) or a postman (A₂).
  - Based on prior knowledge, you reject A₂ because it looks more like a tree.
  - This example illustrates how prior beliefs can influence decision-making.
