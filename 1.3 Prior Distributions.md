---
title: "Bayes' Theorem"
---
# 1.3 Prior Distributions

### Conjugate Priors
- A prior distribution is said to be **conjugate to a likelihood function** if the posterior distribution is of the **same family** as the prior.
- family of distributions: $F$ , being conjugate to a family of sampling distributions $P$ , if the posterior distribution belongs to $F$ for any sample size and any value of observations.
- Example: The Beta distribution is a conjugate prior to the Binomial distribution.

### Non-Informative/Vague Priors
- When we do not have any prior information concerning the parameter of interest, the Uniform prior should be used.
- The **posterior distribution** is **proportional to the likelihood function** when using a Uniform prior.
- **Non-linear transformations** of the parameter can result in a **non-Uniform prior** on the transformed parameter.
  
#### Jeffreys' Prior
- Based on an **invariance** rule for **one-to-one** transformations.
- Suggested by Jeffreys, this prior is based on the **Fisher's Information**.
- Fisher's Information, $I(\theta|x)$, is defined as:
  <div align="center">
  $I(\theta|x) = E\left[ \left( \frac{\partial}{\partial \theta} \log f(x|\theta) \right)^2 \right] = -E\left[ \frac{\partial^2 \log f(x|\theta)}{\partial \theta^2} \right]$
  </div>
  
- Jeffreys' prior is given by:
  <div align="center">
  \[ p(\theta) \propto \sqrt{I(\theta|x)} \]
  </div>
- Consider the parameter $\phi$, which is a transformation of the parameter $\theta$, i.e. $\phi = h(\theta)$. If we specify, $p(\theta) \propto \sqrt{I(\theta | \mathbf{x})}$ ,
then,
  <div align="center">
   $p(\phi) \propto \sqrt{I(\phi | \mathbf{x})}$
  </div> 

- $n$ independent observations $\mathbf{x} = \{x_1, \ldots, x_n\}$ generated from the same distribution with pdf $f$ ($X \sim f$). Then, it can be easily shown that Fisher's information is given by, $I(\theta | \mathbf{x}) = n I(\theta | x_i)$ .

- Jeffreys' prior can be extended to the case where there are several unknown parameters. Then, Fisher's information is defined as the matrix, with the element in row $i$ and column $j$ given by,
  <div align="center">
    $(I(\theta | \mathbf{x}))_{i,j} = -\mathbb{E}\left(\frac{\partial^2 \log f(\mathbf{x} | \theta)}{\partial \theta_i \partial \theta_j}\right)$
  </div>

The prior is specified as, $p(\theta) \propto \sqrt{\det I(\theta | \mathbf{x})}$

### Informative Priors
- In many experiments or problems, experts will have an understanding of the system and hence some ideas as to what value the parameter(s) should take.
- This information needs to be independent of the observed data.
- It is useful for experts to specify their prior beliefs before conducting the given experiment.
- A Bayesian analysis of the subsequent data permits the formal inclusion of these prior beliefs within the analysis of the data within a robust framework via the specification of informative priors.
- However, in general an expert will not specify their prior beliefs in the form of a statistical distribution.
- To form a prior distribution that reflects an expert's prior knowledge a process of elicitation is used.
- Elicitation is the process of extracting prior knowledge in a suitable manner to permit the formulation of a suitable prior distribution that represents this information as accurately as possible.
- In general, the goal of this process is to identify and quantify the key aspects of prior knowledge, since the expert will find these easier to specify.
- Once summaries have been elicited, there will be more than one distributions that fit those summaries.
- In practice, once we elicit some key aspects of prior knowledge, we fit them to a distribution.
- We typically discuss the distribution with the expert, for example, showing a plot of the distribution and discussing several aspects or consequences of the given prior.
- If the expert is happy with the feedback, we consider it to be the prior; else discuss why the distribution does not represent the expert's beliefs and adjust the prior distribution accordingly and repeat.
