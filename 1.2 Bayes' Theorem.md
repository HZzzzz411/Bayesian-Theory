---
title: "Bayes' Theorem"
---
# 1.2 Bayes' Theorem

### Discrete Case

For discrete events $A$ and $B$ with $P(B) > 0$, Bayes' Theorem states:
$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

Here, $P(B)$ can be expressed using the **law of total probability**. Suppose \(A_i\) are **mutually exclusive** and exhaustive events, then:
$P(B) = \sum_{i=1}^{n} P(B|A_i)P(A_i)$

Thus, an alternative form of Bayes' Theorem is:
$P(A_j|B) = \frac{P(B|A_j)P(A_j)}{\sum_{i=1}^{n} P(B|A_i)P(A_i)}$


### Continuous Case (Single Parameter)

For continuous parameters, Bayes' Theorem is equally well defined. Suppose a parameter $\theta$ with a prior distribution $p(\theta)$ and observe data $x$ from a known probability density function $f(x|\theta)$. Bayes' Theorem states:
$\pi(\theta|x) = \frac{f(x|\theta)p(\theta)}{f(x)}$

Here, $f(x)^{-1}$ is normalizing constant:
$f(x) = \int_{\Theta} f(x|\theta)p(\theta)d\theta$

In practice, the posterior distribution is often expressed up to proportionality:
$\pi(\theta|x) \propto f(x|\theta)p(\theta)$

- **likelihood principle**: Bayesian inference obeys.

   For a given sample of data, x, any probability models that lead to the same likelihood function will yield the same inference for $\theta$. 

   (data only affects the posterior via the likelihood $f(x|\theta)$, prior is independent of the data.)
