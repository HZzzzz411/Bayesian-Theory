# 1.2 Bayes' Theorem

### Discrete Case

For discrete events $A$ and $B$ with $P(B) > 0$, Bayes' Theorem states:
$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

Here, \(P(B)\) can be expressed using the law of total probability. Suppose \(A_i\) are mutually exclusive and exhaustive events, then:
\[ P(B) = \sum_{i=1}^{n} P(B|A_i)P(A_i) \]

Thus, an alternative form of Bayes' Theorem is:
\[ P(A_j|B) = \frac{P(B|A_j)P(A_j)}{\sum_{i=1}^{n} P(B|A_i)P(A_i)} \]

#### Example: Hepatitis C Test
Suppose a test for Hepatitis C is given to a population where only 10% are positive. The test is 95% accurate for those with the disease and 80% accurate for those without. We want to find the probability of a false negative or false positive, and the probability that an individual does not have Hepatitis C given two positive test results.

Let:
- \(A\): The event that an individual has Hepatitis C.
- \(B\): The event that the test result is positive.

Given:
- \(P(A) = 0.1\)
- \(P(B|A) = 0.95\)
- \(P(B^c|A^c) = 0.8\)

We calculate:
- **False Negative**:
  \[
  P(A|B^c) = \frac{P(B^c|A)P(A)}{P(B^c)} = \frac{0.05 \times 0.1}{(0.05 \times 0.1) + (0.8 \times 0.9)} = 0.0069
  \]

- **False Positive**:
  \[
  P(A^c|B) = \frac{P(B|A^c)P(A^c)}{P(B)} = \frac{0.2 \times 0.9}{(0.2 \times 0.9) + (0.95 \times 0.1)} = 0.6545
  \]

Given a second positive test result (\(C\)), the probability that the individual does not have Hepatitis C is:
\[
P(A^c|B, C) = \frac{P(B, C|A^c)P(A^c)}{P(B, C)} = \frac{P(B|A^c)P(C|A^c)P(A^c)}{P(B)P(C)} = 0.476
\]

### Continuous Case (Single Parameter)

For continuous parameters, Bayes' Theorem is similarly defined. Suppose we have a parameter \(\theta\) with a prior distribution \(p(\theta)\) and observe data \(x\) from a known probability density function \(f(x|\theta)\). Bayes' Theorem states:
\[ \pi(\theta|x) = \frac{f(x|\theta)p(\theta)}{f(x)} \]

Here, the normalizing constant \(f(x)\) is:
\[ f(x) = \int_{\Theta} f(x|\theta)p(\theta)d\theta \]

In practice, the posterior distribution is often expressed up to proportionality:
\[ \pi(\theta|x) \propto f(x|\theta)p(\theta) \]

#### Example: Exponential Distribution
Suppose we observe data \(x = \{x_1, \ldots, x_n\}\) such that each \(X_i \sim \text{Exp}(\lambda)\). We place a Gamma prior on \(\lambda\):
\[ \lambda \sim \Gamma(\alpha, \beta) \]

The posterior distribution for \(\lambda\) is:
\[
\pi(\lambda|x) \propto f(x|\lambda)p(\lambda) = \prod_{i=1}^{n} \lambda e^{-\lambda x_i} \times \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta \lambda}
\]
\[
\propto \lambda^n e^{-\lambda \sum_{i=1}^{n} x_i} \times \lambda^{\alpha-1} e^{-\beta \lambda} = \lambda^{n+\alpha-1} e^{-\lambda (\sum_{i=1}^{n} x_i + \beta)}
\]
\[
\Rightarrow \lambda|x \sim \Gamma(n+\alpha, \sum_{i=1}^{n} x_i + \beta)
\]

This example demonstrates how the posterior distribution can be derived analytically when using conjugate priors.
